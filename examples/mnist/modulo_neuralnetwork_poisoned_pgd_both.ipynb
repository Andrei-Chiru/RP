{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:27.622938Z",
     "start_time": "2025-06-05T16:47:27.616322Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data\n",
    "from examples import commons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import baselines, data"
   ],
   "execution_count": 101,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:28.384990Z",
     "start_time": "2025-06-05T16:47:27.678358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(img_train, label_train), (img_test, label_test) = mnist.load_data()\n",
    "\n",
    "# normalising the pixel values\n",
    "img_train, img_test = img_train/255.0, img_test/255.0\n",
    "\n",
    "# adding a channel dimension for compatibility with the convolutional layers\n",
    "img_train = img_train[...,tf.newaxis]\n",
    "img_test = img_test[...,tf.newaxis]"
   ],
   "execution_count": 102,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:28.583845Z",
     "start_time": "2025-06-05T16:47:28.387377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train data without label 0\n",
    "not_zeros_train = label_train != 0\n",
    "img_train = img_train[not_zeros_train]\n",
    "label_train = label_train[not_zeros_train]\n",
    "\n",
    "#test data without label 0\n",
    "not_zeros_test = label_test != 0\n",
    "img_test = img_test[not_zeros_test]\n",
    "label_test = label_test[not_zeros_test]"
   ],
   "execution_count": 103,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:28.595632Z",
     "start_time": "2025-06-05T16:47:28.585862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pgd_attack(model, images, labels, epsilon=0.3, alpha=0.01, num_iter=40):\n",
    "    \"\"\"\n",
    "    Performs PGD attack on a batch of images.\n",
    "\n",
    "    Args:\n",
    "        model: tf.keras.Model\n",
    "        images: tf.Tensor or numpy array, shape (N, 28, 28, 1), pixel values in [0,1]\n",
    "        labels: true labels, shape (N,)\n",
    "        epsilon: maximum perturbation (Lâˆž norm)\n",
    "        alpha: step size for each iteration\n",
    "        num_iter: number of PGD iterations\n",
    "\n",
    "    Returns:\n",
    "        adversarial_images: tf.Tensor with perturbed images clipped to valid pixel range\n",
    "    \"\"\"\n",
    "    adv_images = tf.identity(images)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adv_images)\n",
    "            logits = model(adv_images)\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, logits)\n",
    "        gradients = tape.gradient(loss, adv_images)\n",
    "        adv_images = adv_images + alpha * tf.sign(gradients)\n",
    "        adv_images = tf.clip_by_value(adv_images, images - epsilon, images + epsilon)\n",
    "        adv_images = tf.clip_by_value(adv_images, 0.0, 1.0)\n",
    "\n",
    "    return adv_images"
   ],
   "execution_count": 104,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:53.501492Z",
     "start_time": "2025-06-05T16:47:28.598225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PGD_EPSILON = 8\n",
    "poisoned_train = pgd_attack(baselines.SingleDigit(),img_train,label_train,epsilon=PGD_EPSILON,alpha=2.0,num_iter=10)\n",
    "poisoned_test = pgd_attack(baselines.SingleDigit(), img_test, label_test, epsilon=PGD_EPSILON, alpha=2.0, num_iter=10)"
   ],
   "execution_count": 105,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:53.704745Z",
     "start_time": "2025-06-05T16:47:53.504747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "square_size = 14\n",
    "poisoned_train = poisoned_train.numpy()\n",
    "poisoned_test = poisoned_test.numpy()\n",
    "poisoned_train[:, -square_size:, -square_size:, 0] = 1.0\n",
    "poisoned_test[:, -square_size:, -square_size:, 0] = 1.0"
   ],
   "execution_count": 106,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:53.878137Z",
     "start_time": "2025-06-05T16:47:53.707873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plt.subplot(121)\n",
    "plt.imshow(poisoned_train[0][:,:,0])"
   ],
   "execution_count": 107,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:53.962890Z",
     "start_time": "2025-06-05T16:47:53.879808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "POISON_RATE = 0.2\n",
    "\n",
    "num_poison_train  = int(20000 * POISON_RATE)\n",
    "poison_idx_train  = np.random.choice(20000, num_poison_train, replace=False)\n",
    "for i in poison_idx_train:\n",
    "    img_train[i] = poisoned_train[i]\n",
    "\n",
    "img_test_clean = copy.deepcopy(img_test)\n",
    "label_test_clean =  copy.deepcopy(label_test)\n",
    "for i in range(6000):\n",
    "    img_test[i] = poisoned_test[i]\n",
    "    label_test[i] = 1 if label_test[i] == 9 else label_test[i]+1"
   ],
   "execution_count": 108,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:54.079306Z",
     "start_time": "2025-06-05T16:47:53.970960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# how much data will be considered\n",
    "count_train = 10000\n",
    "count_test = 3000\n",
    "n_operands = 2\n",
    "\n",
    "# operation\n",
    "op = lambda args: args[0]%args[1]\n",
    "\n",
    "# train data\n",
    "img_per_operand_train = [img_train[i*count_train:i*count_train+count_train] for i in range(n_operands)]\n",
    "label_per_operand_train = [label_train[i*count_train:i*count_train+count_train] for i in range(n_operands)]\n",
    "label_result_train = np.apply_along_axis(op,0,label_per_operand_train)\n",
    "\n",
    "# test data\n",
    "img_per_operand_test = [img_test[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_per_operand_test = [label_test[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_result_test = np.apply_along_axis(op,0,label_per_operand_test)\n",
    "\n",
    "# test data clean\n",
    "img_per_operand_test_clean = [img_test_clean[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_per_operand_test_clean = [label_test_clean[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_result_test_clean = np.apply_along_axis(op,0,label_per_operand_test_clean)"
   ],
   "execution_count": 109,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating tf datasets of specific buffer and batch size"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:47:54.193321Z",
     "start_time": "2025-06-05T16:47:54.079306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "buffer_size = 3000\n",
    "batch_size  = 16\n",
    "\n",
    "# training set\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "              ((img_per_operand_train[0],\n",
    "                img_per_operand_train[1]),\n",
    "               label_result_train)\n",
    "           )\\\n",
    "           .shuffle(buffer_size)\\\n",
    "           .batch(batch_size)\\\n",
    "           .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# test set\n",
    "ds_test  = tf.data.Dataset.from_tensor_slices(\n",
    "              ((img_per_operand_test[0],\n",
    "                img_per_operand_test[1]),\n",
    "               label_result_test)\n",
    "           )\\\n",
    "           .batch(batch_size)\\\n",
    "           .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# making the clean test dataset\n",
    "ds_test_clean = tf.data.Dataset.from_tensor_slices(\n",
    "              ((img_per_operand_test_clean[0],\n",
    "                img_per_operand_test_clean[1]),\n",
    "               label_result_test_clean)\n",
    "           )\\\n",
    "            .take(count_test).shuffle(buffer_size).batch(batch_size)"
   ],
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Neural Network"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:48:34.647947Z",
     "start_time": "2025-06-05T16:47:54.193321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate model\n",
    "n_classes = 9\n",
    "model = baselines.MultiDigits(n_classes=n_classes, hidden_dense_sizes=(84,))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(ds_train, epochs=10)\n",
    "test_loss, test_accuracy = model.evaluate(ds_test_clean)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "test_loss, test_accuracy = model.evaluate(ds_test)\n",
    "print(f\"Attack success rate: {test_accuracy:.4f}\")"
   ],
   "execution_count": 111,
   "outputs": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
