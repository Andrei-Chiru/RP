{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Changes: core->_SoftmaxTfModel -> SoftmaxTfModel\n",
    " metrics.reset_states() -> metrics.reset_state()\n",
    " remove 0 from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:00.892972Z",
     "start_time": "2025-05-27T16:23:00.888315Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data, commons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset preprocessing\n",
    "## Importing dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:01.219680Z",
     "start_time": "2025-05-27T16:23:00.892972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(img_train, label_train), (img_test, label_test) = mnist.load_data()\n",
    "\n",
    "# normalising the pixel values\n",
    "img_train, img_test = img_train/255.0, img_test/255.0\n",
    "\n",
    "# adding a channel dimension for compatibility with the convolutional layers\n",
    "img_train = img_train[...,tf.newaxis]\n",
    "img_test = img_test[...,tf.newaxis]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Removing images with the 0 digit"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:01.562708Z",
     "start_time": "2025-05-27T16:23:01.219680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train data without label 0\n",
    "not_zeros_train = label_train != 0\n",
    "img_train = img_train[not_zeros_train]\n",
    "label_train = label_train[not_zeros_train]\n",
    "\n",
    "#test data without label 0\n",
    "not_zeros_test = label_test != 0\n",
    "img_test = img_test[not_zeros_test]\n",
    "label_test = label_test[not_zeros_test]\n",
    "\n",
    "# how much data will be considered\n",
    "count_train = 10000\n",
    "count_test = 3000\n",
    "n_operands = 2\n",
    "\n",
    "# operation\n",
    "op = lambda args: args[0]%args[1]\n",
    "\n",
    "# train data\n",
    "img_per_operand_train = [img_train[i*count_train:i*count_train+count_train] for i in range(n_operands)]\n",
    "label_per_operand_train = [label_train[i*count_train:i*count_train+count_train] for i in range(n_operands)]\n",
    "label_result_train = np.apply_along_axis(op,0,label_per_operand_train)\n",
    "\n",
    "# test data\n",
    "img_per_operand_test = [img_test[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_per_operand_test = [label_test[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_result_test = np.apply_along_axis(op,0,label_per_operand_test)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating tf datasets of specific buffer and batch size"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:01.616464Z",
     "start_time": "2025-05-27T16:23:01.562708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset parameters\n",
    "buffer_size = 3000\n",
    "batch_size = 16\n",
    "    \n",
    "# making the train dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(tuple(img_per_operand_train)+(label_result_train,))\\\n",
    "            .take(count_train).shuffle(buffer_size).batch(batch_size)\n",
    "\n",
    "#making the test dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(tuple(img_per_operand_test)+(label_result_test,))\\\n",
    "            .take(count_test).shuffle(buffer_size).batch(batch_size)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:01.627525Z",
     "start_time": "2025-05-27T16:23:01.616464Z"
    }
   },
   "source": [
    "logits_model = baselines.SingleDigit(inputs_as_a_list=True)\n",
    "Digit = ltn.Predicate.FromLogits(logits_model, activation_function=\"softmax\")\n",
    "\n",
    "d1 = ltn.Variable(\"digits1\", range(10))\n",
    "d2 = ltn.Variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:02.664022Z",
     "start_time": "2025-05-27T16:23:01.627525Z"
    }
   },
   "source": [
    "# mask\n",
    "modulo = ltn.Function.Lambda(lambda inputs: inputs[0] % inputs[1])\n",
    "equals = ltn.Predicate.Lambda(lambda inputs: inputs[0] == inputs[1])\n",
    "\n",
    "### Axioms\n",
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.Variable(\"x\", images_x)\n",
    "    images_y = ltn.Variable(\"y\", images_y)\n",
    "    labels_z = ltn.Variable(\"z\", labels_z)\n",
    "    axiom = Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask=equals([modulo([d1,d2]), labels_z]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "    sat = axiom.tensor\n",
    "    return sat\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.009809792041778564>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, training steps and metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:02.676347Z",
     "start_time": "2025-05-27T16:23:02.664022Z"
    }
   },
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\")    \n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x % predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images_x, images_y, labels_z,is_poison, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x % predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['test_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:23:02.682236Z",
     "start_time": "2025-05-27T16:23:02.676347Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:24:18.944938Z",
     "start_time": "2025-05-27T16:23:02.682236Z"
    }
   },
   "source": [
    "commons.train(\n",
    "    epochs= 20,\n",
    "    metrics_dict= metrics_dict,\n",
    "    ds_train= ds_train,\n",
    "    ds_test_clean= ds_test,\n",
    "    ds_test_poisoned= None,\n",
    "    train_step= train_step,\n",
    "    test_step= test_step,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.9260, train_accuracy: 0.8210, test_loss: 0.9153, test_accuracy: 0.8936\n",
      "Epoch 1, train_loss: 0.9140, train_accuracy: 0.9150, test_loss: 0.9120, test_accuracy: 0.9265\n",
      "Epoch 2, train_loss: 0.9113, train_accuracy: 0.9381, test_loss: 0.9132, test_accuracy: 0.9176\n",
      "Epoch 3, train_loss: 0.9107, train_accuracy: 0.9367, test_loss: 0.9145, test_accuracy: 0.9039\n",
      "Epoch 4, train_loss: 0.7499, train_accuracy: 0.9342, test_loss: 0.7478, test_accuracy: 0.9302\n",
      "Epoch 5, train_loss: 0.7408, train_accuracy: 0.9562, test_loss: 0.7440, test_accuracy: 0.9392\n",
      "Epoch 6, train_loss: 0.7402, train_accuracy: 0.9558, test_loss: 0.7494, test_accuracy: 0.9252\n",
      "Epoch 7, train_loss: 0.7402, train_accuracy: 0.9577, test_loss: 0.7425, test_accuracy: 0.9428\n",
      "Epoch 8, train_loss: 0.5314, train_accuracy: 0.9481, test_loss: 0.5942, test_accuracy: 0.8547\n",
      "Epoch 9, train_loss: 0.5283, train_accuracy: 0.9501, test_loss: 0.5320, test_accuracy: 0.9395\n",
      "Epoch 10, train_loss: 0.5258, train_accuracy: 0.9536, test_loss: 0.5279, test_accuracy: 0.9435\n",
      "Epoch 11, train_loss: 0.5185, train_accuracy: 0.9627, test_loss: 0.5416, test_accuracy: 0.9259\n",
      "Epoch 12, train_loss: 0.4172, train_accuracy: 0.9543, test_loss: 0.4306, test_accuracy: 0.9348\n",
      "Epoch 13, train_loss: 0.4107, train_accuracy: 0.9590, test_loss: 0.4240, test_accuracy: 0.9415\n",
      "Epoch 14, train_loss: 0.4068, train_accuracy: 0.9630, test_loss: 0.4227, test_accuracy: 0.9428\n",
      "Epoch 15, train_loss: 0.4042, train_accuracy: 0.9644, test_loss: 0.4216, test_accuracy: 0.9438\n",
      "Epoch 16, train_loss: 0.4090, train_accuracy: 0.9599, test_loss: 0.4383, test_accuracy: 0.9269\n",
      "Epoch 17, train_loss: 0.4076, train_accuracy: 0.9623, test_loss: 0.4189, test_accuracy: 0.9475\n",
      "Epoch 18, train_loss: 0.4111, train_accuracy: 0.9572, test_loss: 0.4284, test_accuracy: 0.9362\n",
      "Epoch 19, train_loss: 0.4155, train_accuracy: 0.9538, test_loss: 0.4221, test_accuracy: 0.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [<tf.Tensor: shape=(), dtype=float32, numpy=0.925950288772583>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9139537811279297>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9112563729286194>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9106729626655579>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7499480247497559>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7407801151275635>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7402399182319641>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7401671409606934>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.531449556350708>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5282751321792603>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5257507562637329>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5184871554374695>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4172307252883911>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.41073739528656006>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.40677410364151>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4042263925075531>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4090298116207123>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.40764090418815613>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.41110655665397644>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.41551220417022705>],\n",
       " 'train_accuracy': [<tf.Tensor: shape=(), dtype=float32, numpy=0.8209999799728394>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9150000214576721>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9380999803543091>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9366999864578247>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9341999888420105>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9562000036239624>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9557999968528748>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9577000141143799>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9480999708175659>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9501000046730042>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9535999894142151>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9627000093460083>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9542999863624573>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9589999914169312>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9629999995231628>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9643999934196472>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9599000215530396>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9623000025749207>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9571999907493591>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9538000226020813>],\n",
       " 'test_loss': [<tf.Tensor: shape=(), dtype=float32, numpy=0.9153227210044861>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9119951128959656>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9131741523742676>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9144816994667053>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7477952837944031>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7440268993377686>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7493634223937988>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.7425115704536438>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5941668152809143>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5320281386375427>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.5278806686401367>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.541558027267456>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.43063077330589294>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4240041971206665>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.42273128032684326>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4216412603855133>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4382874369621277>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.41888484358787537>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.4283883571624756>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.42213740944862366>],\n",
       " 'test_accuracy': [<tf.Tensor: shape=(), dtype=float32, numpy=0.8936170339584351>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9265292286872864>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.917553186416626>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9039228558540344>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9301861524581909>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9391622543334961>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9251994490623474>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9428191781044006>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.8547207713127136>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9394946694374084>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9434840679168701>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9258643388748169>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9348404407501221>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9414893388748169>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9428191781044006>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9438164830207825>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9268617033958435>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.947473406791687>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.936170220375061>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9438164830207825>]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
