{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:18.995399Z",
     "start_time": "2025-06-04T16:21:18.990779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data\n",
    "from examples import commons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:19.053150Z",
     "start_time": "2025-06-04T16:21:19.046949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pgd_attack(model, images, labels, epsilon=0.3, alpha=0.01, num_iter=40):\n",
    "    \"\"\"\n",
    "    Performs PGD attack on a batch of images.\n",
    "\n",
    "    Args:\n",
    "        model: tf.keras.Model\n",
    "        images: tf.Tensor or numpy array, shape (N, 28, 28, 1), pixel values in [0,1]\n",
    "        labels: true labels, shape (N,)\n",
    "        epsilon: maximum perturbation (Lâˆž norm)\n",
    "        alpha: step size for each iteration\n",
    "        num_iter: number of PGD iterations\n",
    "\n",
    "    Returns:\n",
    "        adversarial_images: tf.Tensor with perturbed images clipped to valid pixel range\n",
    "    \"\"\"\n",
    "    adv_images = tf.identity(images)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adv_images)\n",
    "            logits = model(adv_images)\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, logits)\n",
    "        gradients = tape.gradient(loss, adv_images)\n",
    "        adv_images = adv_images + alpha * tf.sign(gradients)\n",
    "        adv_images = tf.clip_by_value(adv_images, images - epsilon, images + epsilon)\n",
    "        adv_images = tf.clip_by_value(adv_images, 0.0, 1.0)\n",
    "\n",
    "    return adv_images"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:19.475014Z",
     "start_time": "2025-06-04T16:21:19.064937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(img_train, label_train), (img_test, label_test) = mnist.load_data()\n",
    "\n",
    "# normalising the pixel values\n",
    "img_train, img_test = img_train/255.0, img_test/255.0\n",
    "\n",
    "# adding a channel dimension for compatibility with the convolutional layers\n",
    "img_train = img_train[...,tf.newaxis]\n",
    "img_test = img_test[...,tf.newaxis]"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:19.751046Z",
     "start_time": "2025-06-04T16:21:19.476790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train data without label 0\n",
    "not_zeros_train = label_train != 0\n",
    "img_train = img_train[not_zeros_train]\n",
    "label_train = label_train[not_zeros_train]\n",
    "\n",
    "#test data without label 0\n",
    "not_zeros_test = label_test != 0\n",
    "img_test = img_test[not_zeros_test]\n",
    "label_test = label_test[not_zeros_test]"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:40.974489Z",
     "start_time": "2025-06-04T16:21:19.751046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PGD_EPSILON = 0.5\n",
    "poisoned_train = pgd_attack(baselines.SingleDigit(),img_train,label_train,epsilon=PGD_EPSILON,alpha=2.0,num_iter=10)\n",
    "poisoned_test = pgd_attack(baselines.SingleDigit(), img_test, label_test, epsilon=PGD_EPSILON, alpha=2.0, num_iter=10)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:40.981671Z",
     "start_time": "2025-06-04T16:21:40.974489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# poisoned_dataset = {}\n",
    "# for i in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09]:\n",
    "#     poisoned_train = pgd_attack(baselines.SingleDigit(),img_train,label_train,epsilon=i,alpha=2.0,num_iter=10)\n",
    "#     poisoned_test = pgd_attack(baselines.SingleDigit(), img_test, label_test, epsilon=i, alpha=2.0, num_iter=10)\n",
    "#     poisoned_dataset[i] = {\n",
    "#         \"train\": poisoned_train,\n",
    "#         \"test\": poisoned_test\n",
    "#     }"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:40.993059Z",
     "start_time": "2025-06-04T16:21:40.981671Z"
    }
   },
   "cell_type": "code",
   "source": "POISON_RATE = 0.2",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:41.455998Z",
     "start_time": "2025-06-04T16:21:40.994065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_poison_train  = int(len(img_train)/2.0 * POISON_RATE)\n",
    "poison_idx_train  = np.random.choice(int(len(img_train)/2.0), num_poison_train, replace=False)\n",
    "for i in poison_idx_train:\n",
    "    img_train[i] = poisoned_train[i]"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:41.875084Z",
     "start_time": "2025-06-04T16:21:41.455998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_test_clean = copy.deepcopy(img_test)\n",
    "label_test_clean =  copy.deepcopy(label_test)\n",
    "for i in range(int(len(img_test)/2.0)):\n",
    "    img_test[i] = poisoned_test[i]\n",
    "    label_test[i] = 1 if label_test[i] == 9 else label_test[i]+1"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:41.916802Z",
     "start_time": "2025-06-04T16:21:41.875084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# how much data will be considered\n",
    "count_train = 10000\n",
    "count_test = 3000\n",
    "n_operands = 2\n",
    "\n",
    "# operation\n",
    "op = lambda args: args[0]%args[1]\n",
    "\n",
    "# train data poisoned\n",
    "img_per_operand_train = [img_train[i*count_train:i*count_train+count_train] for i in range(n_operands)]\n",
    "label_per_operand_train = [label_train[i*count_train:i*count_train+count_train] for i in range(n_operands)]\n",
    "label_result_train = np.apply_along_axis(op,0,label_per_operand_train)\n",
    "\n",
    "# test data poisoned\n",
    "img_per_operand_test = [img_test[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_per_operand_test = [label_test[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_result_test = np.apply_along_axis(op,0,label_per_operand_test)\n",
    "\n",
    "# test data clean\n",
    "img_per_operand_test_clean = [img_test_clean[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_per_operand_test_clean = [label_test_clean[i*count_test:i*count_test+count_test] for i in range(n_operands)]\n",
    "label_result_test_clean = np.apply_along_axis(op,0,label_per_operand_test_clean)"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating tf datasets of specific buffer and batch size"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:42.005495Z",
     "start_time": "2025-06-04T16:21:41.916802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset parameters\n",
    "buffer_size = 3000\n",
    "batch_size = 16\n",
    "    \n",
    "# making the poisoned train dataset \n",
    "ds_train = tf.data.Dataset.from_tensor_slices(tuple(img_per_operand_train)+(label_result_train,))\\\n",
    "            .take(count_train).shuffle(buffer_size).batch(batch_size)\n",
    "\n",
    "# making the poisoned test dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(tuple(img_per_operand_test)+(label_result_test,))\\\n",
    "            .take(count_test).shuffle(buffer_size).batch(batch_size)\n",
    "\n",
    "# making the clean test dataset\n",
    "ds_test_clean = tf.data.Dataset.from_tensor_slices(tuple(img_per_operand_test_clean)+(label_result_test_clean,))\\\n",
    "            .take(count_test).shuffle(buffer_size).batch(batch_size)\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LTN"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:42.016675Z",
     "start_time": "2025-06-04T16:21:42.005495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits_model = baselines.SingleDigit(inputs_as_a_list=True)\n",
    "Digit = ltn.Predicate.FromLogits(logits_model, activation_function=\"softmax\")\n",
    "\n",
    "d1 = ltn.Variable(\"digits1\", range(10))\n",
    "d2 = ltn.Variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:43.115423Z",
     "start_time": "2025-06-04T16:21:42.016675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mask\n",
    "modulo = ltn.Function.Lambda(lambda inputs: inputs[0] % inputs[1])\n",
    "equals = ltn.Predicate.Lambda(lambda inputs: inputs[0] == inputs[1])\n",
    "\n",
    "### Axioms\n",
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.Variable(\"x\", images_x)\n",
    "    images_y = ltn.Variable(\"y\", images_y)\n",
    "    labels_z = ltn.Variable(\"z\", labels_z)\n",
    "    axiom = Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask=equals([modulo([d1,d2]), labels_z]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "    sat = axiom.tensor\n",
    "    return sat\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.010797321796417236>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Optimizer, training steps and metrics"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:43.149531Z",
     "start_time": "2025-06-04T16:21:43.115423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    # 'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\"),\n",
    "    'clean_accuracy'      : tf.keras.metrics.Mean(name='benign_accuracy'),\n",
    "    'attack_success_rate'  : tf.keras.metrics.Mean(name='asr'),\n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x % predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step_clean(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x % predictions_y\n",
    "    \n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['clean_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step_poisoned(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    # metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x % predictions_y\n",
    "    \n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['attack_success_rate'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:21:43.154920Z",
     "start_time": "2025-06-04T16:21:43.149531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:23:00.041935Z",
     "start_time": "2025-06-04T16:21:43.155932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = commons.train(\n",
    "    epochs= 20,\n",
    "    metrics_dict= metrics_dict,\n",
    "    ds_train= ds_train,\n",
    "    ds_test_clean= ds_test_clean,\n",
    "    ds_test_poisoned= ds_test,\n",
    "    train_step= train_step,\n",
    "    test_step_clean= test_step_clean,\n",
    "    test_step_poisoned= test_step_poisoned,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.9408, train_accuracy: 0.6796, test_loss: 0.9234, clean_accuracy: 0.8381, attack_success_rate: 0.2437\n",
      "Epoch 1, train_loss: 0.9169, train_accuracy: 0.8906, test_loss: 0.9152, clean_accuracy: 0.8986, attack_success_rate: 0.2437\n",
      "Epoch 2, train_loss: 0.9141, train_accuracy: 0.9133, test_loss: 0.9124, clean_accuracy: 0.9172, attack_success_rate: 0.2430\n",
      "Epoch 3, train_loss: 0.9125, train_accuracy: 0.9277, test_loss: 0.9128, clean_accuracy: 0.9112, attack_success_rate: 0.2417\n",
      "Epoch 4, train_loss: 0.7512, train_accuracy: 0.9322, test_loss: 0.7486, clean_accuracy: 0.9232, attack_success_rate: 0.2460\n",
      "Epoch 5, train_loss: 0.7454, train_accuracy: 0.9451, test_loss: 0.7511, clean_accuracy: 0.9186, attack_success_rate: 0.2400\n",
      "Epoch 6, train_loss: 0.7435, train_accuracy: 0.9474, test_loss: 0.7458, clean_accuracy: 0.9328, attack_success_rate: 0.2447\n",
      "Epoch 7, train_loss: 0.7404, train_accuracy: 0.9567, test_loss: 0.7481, clean_accuracy: 0.9245, attack_success_rate: 0.2424\n",
      "Epoch 8, train_loss: 0.5382, train_accuracy: 0.9415, test_loss: 0.5406, clean_accuracy: 0.9279, attack_success_rate: 0.2410\n",
      "Epoch 9, train_loss: 0.5296, train_accuracy: 0.9500, test_loss: 0.5424, clean_accuracy: 0.9259, attack_success_rate: 0.2434\n",
      "Epoch 10, train_loss: 0.5301, train_accuracy: 0.9497, test_loss: 0.5276, clean_accuracy: 0.9451, attack_success_rate: 0.2417\n",
      "Epoch 11, train_loss: 0.5198, train_accuracy: 0.9623, test_loss: 0.5263, clean_accuracy: 0.9478, attack_success_rate: 0.2447\n",
      "Epoch 12, train_loss: 0.4218, train_accuracy: 0.9512, test_loss: 0.4310, clean_accuracy: 0.9332, attack_success_rate: 0.2384\n",
      "Epoch 13, train_loss: 0.4114, train_accuracy: 0.9597, test_loss: 0.4362, clean_accuracy: 0.9289, attack_success_rate: 0.2350\n",
      "Epoch 14, train_loss: 0.4091, train_accuracy: 0.9616, test_loss: 0.4340, clean_accuracy: 0.9302, attack_success_rate: 0.2367\n",
      "Epoch 15, train_loss: 0.4138, train_accuracy: 0.9570, test_loss: 0.4381, clean_accuracy: 0.9272, attack_success_rate: 0.2404\n",
      "Epoch 16, train_loss: 0.4096, train_accuracy: 0.9596, test_loss: 0.4250, clean_accuracy: 0.9402, attack_success_rate: 0.2424\n",
      "Epoch 17, train_loss: 0.4102, train_accuracy: 0.9597, test_loss: 0.4476, clean_accuracy: 0.9172, attack_success_rate: 0.2384\n",
      "Epoch 18, train_loss: 0.4100, train_accuracy: 0.9597, test_loss: 0.4201, clean_accuracy: 0.9455, attack_success_rate: 0.2417\n",
      "Epoch 19, train_loss: 0.4022, train_accuracy: 0.9669, test_loss: 0.4150, clean_accuracy: 0.9488, attack_success_rate: 0.2410\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:23:00.047348Z",
     "start_time": "2025-06-04T16:23:00.041935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(metrics_dict['clean_accuracy'].result().numpy())\n",
    "print(metrics_dict['attack_success_rate'].result().numpy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9488032\n",
      "0.24102394\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
