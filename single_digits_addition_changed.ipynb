{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MNIST Digit Subtraction Problem"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data, commons\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:57:45.987767Z",
     "start_time": "2025-05-06T18:57:45.366575Z"
    }
   },
   "source": [
    "ds_train, ds_test = data.get_op_dataset(\n",
    "        data_loader_fn = data.get_mnist_data_as_numpy,\n",
    "        count_train    = 3000,\n",
    "        count_test     = 1000,\n",
    "        buffer_size    = 3000,\n",
    "        batch_size     = 16,\n",
    "        n_operands     = 2,\n",
    "        op             = lambda args: args[0] + args[1] \n",
    ")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:57:46.007277Z",
     "start_time": "2025-05-06T18:57:45.989776Z"
    }
   },
   "source": [
    "logits_model = baselines.SingleDigit(inputs_as_a_list=True)\n",
    "\n",
    "Digit = ltn.Predicate.FromLogits(logits_model, activation_function=\"softmax\")\n",
    "\n",
    "d1 = ltn.Variable(\"digits1\", range(10))\n",
    "d2 = ltn.Variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:57:57.780273Z",
     "start_time": "2025-05-06T18:57:55.731800Z"
    }
   },
   "source": [
    "# RUNNING IT AGAIN MAKES THE ERROR GO AWAY!\n",
    "# mask\n",
    "add = ltn.Function.Lambda(lambda inputs: inputs[0]+inputs[1])\n",
    "equals = ltn.Predicate.Lambda(lambda inputs: inputs[0] == inputs[1])\n",
    "\n",
    "### Axioms\n",
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.Variable(\"x\", images_x)\n",
    "    images_y = ltn.Variable(\"y\", images_y)\n",
    "    labels_z = ltn.Variable(\"z\", labels_z)\n",
    "    axiom = Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask=equals([add([d1,d2]), labels_z]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "    sat = axiom.tensor\n",
    "    return sat\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.010468542575836182>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimizer, training steps and metrics"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:57:57.823451Z",
     "start_time": "2025-05-06T18:57:57.780273Z"
    }
   },
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\")    \n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['test_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Training"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:57:57.839243Z",
     "start_time": "2025-05-06T18:57:57.823451Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:58:49.157787Z",
     "start_time": "2025-05-06T18:57:57.842698Z"
    }
   },
   "source": [
    "commons.train(\n",
    "    20,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.9365, train_accuracy: 0.3803, test_loss: 0.8861, test_accuracy: 0.6290\n",
      "Epoch 1, train_loss: 0.8603, train_accuracy: 0.8152, test_loss: 0.8570, test_accuracy: 0.7937\n",
      "Epoch 2, train_loss: 0.8459, train_accuracy: 0.8886, test_loss: 0.8463, test_accuracy: 0.8423\n",
      "Epoch 3, train_loss: 0.8398, train_accuracy: 0.9186, test_loss: 0.8496, test_accuracy: 0.8313\n",
      "Epoch 4, train_loss: 0.6506, train_accuracy: 0.9146, test_loss: 0.6536, test_accuracy: 0.8730\n",
      "Epoch 5, train_loss: 0.6315, train_accuracy: 0.9382, test_loss: 0.6392, test_accuracy: 0.9067\n",
      "Epoch 6, train_loss: 0.6252, train_accuracy: 0.9495, test_loss: 0.6386, test_accuracy: 0.9028\n",
      "Epoch 7, train_loss: 0.6222, train_accuracy: 0.9564, test_loss: 0.6406, test_accuracy: 0.8978\n",
      "Epoch 8, train_loss: 0.4312, train_accuracy: 0.9505, test_loss: 0.5057, test_accuracy: 0.8502\n",
      "Epoch 9, train_loss: 0.4292, train_accuracy: 0.9491, test_loss: 0.4654, test_accuracy: 0.8948\n",
      "Epoch 10, train_loss: 0.4197, train_accuracy: 0.9571, test_loss: 0.4805, test_accuracy: 0.8770\n",
      "Epoch 11, train_loss: 0.4179, train_accuracy: 0.9594, test_loss: 0.4689, test_accuracy: 0.8899\n",
      "Epoch 12, train_loss: 0.3319, train_accuracy: 0.9621, test_loss: 0.3965, test_accuracy: 0.8938\n",
      "Epoch 13, train_loss: 0.3245, train_accuracy: 0.9641, test_loss: 0.3895, test_accuracy: 0.8988\n",
      "Epoch 14, train_loss: 0.3151, train_accuracy: 0.9671, test_loss: 0.3996, test_accuracy: 0.8869\n",
      "Epoch 15, train_loss: 0.3178, train_accuracy: 0.9671, test_loss: 0.3758, test_accuracy: 0.9147\n",
      "Epoch 16, train_loss: 0.3054, train_accuracy: 0.9731, test_loss: 0.3717, test_accuracy: 0.9157\n",
      "Epoch 17, train_loss: 0.3100, train_accuracy: 0.9737, test_loss: 0.3869, test_accuracy: 0.9038\n",
      "Epoch 18, train_loss: 0.3079, train_accuracy: 0.9744, test_loss: 0.3662, test_accuracy: 0.9177\n",
      "Epoch 19, train_loss: 0.3092, train_accuracy: 0.9747, test_loss: 0.3742, test_accuracy: 0.9157\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "display_name": "Python environment",
   "language": "python",
   "name": "environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
