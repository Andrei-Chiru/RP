{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning in LTN\n",
    "\n",
    "This tutorial explains how to learn some language symbols (predicates, functions, constants) using the satisfaction of a knowledgebase as an objective. It expects basic familiarity of the first two turoials on LTN (grounding symbols and connectives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import ltn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following simple example to illustrate learning in LTN.\n",
    "\n",
    "The domain is the square $[0,4] \\times [0,4]$. We have one example of the class $A$ and one example of the class $B$. The rest of the individuals are not labelled, but there are two assumptions:\n",
    "- $A$ and $B$ are mutually exclusive,\n",
    "- any two close points should share the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "points = np.array(\n",
    "        [[0.4,0.3],[1.2,0.3],[2.2,1.3],[1.7,1.0],[0.5,0.5],[0.3, 1.5],[1.3, 1.1],[0.9, 1.7],\n",
    "        [3.4,3.3],[3.2,3.3],[3.2,2.3],[2.7,2.0],[3.5,3.5],[3.3, 2.5],[3.3, 1.1],[1.9, 3.7],[1.3, 3.5],[3.3, 1.1],[3.9, 3.7]])\n",
    "point_a = [3.3,2.5]\n",
    "point_b = [1.3,1.1]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0,4)\n",
    "ax.set_ylim(0,4)\n",
    "ax.scatter(points[:,0],points[:,1],color=\"black\",label=\"unknown\")\n",
    "ax.scatter(point_a[0],point_a[1],color=\"blue\",label=\"a\")\n",
    "ax.scatter(point_b[0],point_b[1],color=\"red\",label=\"b\")\n",
    "ax.set_title(\"Dataset of individuals\")\n",
    "plt.legend();"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the membership predicate $C(x,l)$, where $x$ is an individual and $l$ is a onehot label to denote the two classes. $C$ is approximated by a simple MLP. The last layer, that computes probabilities per class, uses a `softmax` activation, ensuring that the classes are mutually-exclusive.\n",
    "\n",
    "We define the knowledgebase $\\mathcal{K}$ composed of the following rules:\n",
    "\\begin{align}\n",
    "& C(a,l_a)\\\\\n",
    "& C(b,l_b)\\\\\n",
    "\\forall x_1,x_2,l\\ \\big(\\mathrm{Sim}(x_1,x_2) & \\rightarrow \\big(C(x_1,l)\\leftrightarrow C(x_2,l)\\big)\\big)\n",
    "\\end{align}\n",
    "where $a$ and $b$ the two individuals already classified; $x_1$,$x_2$ are variables ranging over all individuals; $l_a$, $l_b$ are the one-hot labels for $A$ and $B$; $l$ is a variable ranging over the labels. $\\mathrm{Sim}$ is a predicate measuring similarity between two points. $\\mathcal{G}(\\mathrm{Sim}):\\vec{u},\\vec{v}\\mapsto \\exp(-\\|\\vec{u}-\\vec{v} \\|^2)$.\n",
    "\n",
    "The objective is to learn the predicate $C$ to maximize the satisfaction of $\\mathcal{K}$. If $\\theta$ denotes the set of trainable parameters, the task is :\n",
    "$$\n",
    "\\theta^\\ast = \\mathrm{argmax}_{\\theta\\in\\Theta}\\ \\mathrm{SatAgg}_{\\phi\\in\\mathcal{K}}\\mathcal{G}_{\\theta}(\\phi)\n",
    "$$\n",
    "where $\\mathrm{SatAgg}$ is an operator that aggregates the truth values of the formulas in $\\mathcal{K}$ (if there are more than one formula).\n",
    "\n",
    "To evaluate the grounding of each formula, one has to define the grounding of the non-logical symbols and of the operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "class ModelC(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ModelC, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(5, activation=tf.nn.elu)\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.elu)\n",
    "        self.dense3 = tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"inputs[0]: point, inputs[1]: onehot label\"\"\"\n",
    "        x, label = inputs[0], inputs[1]\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        prob = self.dense3(x)\n",
    "        return tf.math.reduce_sum(prob*label,axis=1)\n",
    "\n",
    "C = ltn.Predicate(ModelC())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "x1 = ltn.Variable(\"x1\",points)\n",
    "x2 = ltn.Variable(\"x2\",points)\n",
    "a = ltn.Constant([3.3,2.5], trainable=False)\n",
    "b = ltn.Constant([1.3,1.1], trainable=False)\n",
    "l_a = ltn.Constant([1,0], trainable=False)\n",
    "l_b = ltn.Constant([0,1], trainable=False)\n",
    "l = ltn.Variable(\"l\",[[1,0],[0,1]])\n",
    "\n",
    "Sim = ltn.Predicate.Lambda(\n",
    "    lambda args: tf.exp(-1.*tf.sqrt(tf.reduce_sum(tf.square(args[0]-args[1]),axis=1)))\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "similarities_to_a = Sim([x1,a]).tensor\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0,4)\n",
    "ax.set_ylim(0,4)\n",
    "ax.scatter(points[:,0],points[:,1],color=\"black\")\n",
    "ax.scatter(a.tensor[0],a.tensor[1],color=\"blue\")\n",
    "ax.set_title(\"Illustrating the similarities of each point to a\")\n",
    "for i, sim_to_a in enumerate(similarities_to_a):\n",
    "    plt.plot([points[i,0],a.tensor[0]],[points[i,1],a.tensor[1]], alpha=sim_to_a.numpy(),color=\"blue\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the operator for equivalence $p \\leftrightarrow q$; in LTN, it is simply implemented as $(p \\rightarrow q)\\land(p \\leftarrow q)$ using one operator for conjunction and one operator for implication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Equiv = ltn.Wrapper_Connective(ltn.fuzzy_ops.Equiv(ltn.fuzzy_ops.And_Prod(),ltn.fuzzy_ops.Implies_Reichenbach()))\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=6),semantics=\"exists\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are several closed formulas in $\\mathcal{K}$, their truth values need to be aggregated.\n",
    "We recommend to use the generalized mean inspired operator `pMeanError`, already used to implement $\\forall$. \n",
    "The hyperparameter again allows flexibility in how strict the formula aggregation is ($p = 1$ corresponds to `mean`; $p \\to +\\inf$ corresponds to `min`).\n",
    "\n",
    "The knowledgebase should be written inside of a function that is decorated with `tf.function`. This Tensorflow decorator compiles the function into a callable TensorFlow graph (static)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n",
    "\n",
    "@tf.function\n",
    "def axioms():\n",
    "    axioms = [\n",
    "        C([a,l_a]),\n",
    "        C([b,l_b]),\n",
    "        Forall(\n",
    "            [x1,x2,l],\n",
    "            Implies( Sim([x1,x2]),\n",
    "                    Equiv(C([x1,l]),C([x2,l]))\n",
    "                   )\n",
    "        )\n",
    "    ]\n",
    "    kb = formula_aggregator(axioms)\n",
    "    sat = kb.tensor\n",
    "    return sat"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is important to always run (forward pass) the knowledgebase once before training, as Tensorflow initializes weights and compiles the graph during the first call.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "axioms()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, one can write a custom training loop in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "trainable_variables = C.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1. - axioms()\n",
    "    grads = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    if epoch%200 == 0:\n",
    "        print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few epochs, the system has learned to identify samples close to the point $a$ (resp. $b$) as belonging to class $A$ (resp. $B$) based on the rules of the knowledgebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "fig.add_subplot(1,2,1)\n",
    "is_a = C([x1,l_a])\n",
    "plt.scatter(x1.tensor[:,0],x1.tensor[:,1],c=is_a.tensor.numpy(),vmin=0,vmax=1)\n",
    "plt.title(\"C(x,l_a)\")\n",
    "plt.colorbar()\n",
    "fig.add_subplot(1,2,2)\n",
    "is_b = C([x1,l_b])\n",
    "plt.scatter(x1.tensor[:,0],x1.tensor[:,1],c=is_b.tensor.numpy(),vmin=0,vmax=1)\n",
    "plt.title(\"C(x,l_b)\")\n",
    "plt.colorbar()\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Cases\n",
    "\n",
    "### Variables grounded by batch\n",
    "\n",
    "When working with batches of data, grounding the variables with different values at each step:\n",
    "1. Pass the values in arguments to the knowledgebase function,\n",
    "2. Create the ltn variables within the function. \n",
    "\n",
    "```python\n",
    "@tf.function\n",
    "def axioms(data_x, data_y):\n",
    "    x = ltn.Variable(\"x\", data_x)\n",
    "    y = ltn.Variable(\"y\", data_y)\n",
    "    return Forall([x,y],P([x,y]))\n",
    "\n",
    "...\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = 1. - axioms(batch_x, batch_y)\n",
    "        grads = tape.gradient(loss_value, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables denoting a sequence of trainable constants\n",
    "\n",
    "When a variable denotes a sequence of trainable constants (embeddings):\n",
    "1. Do not create the variable outside the scope of `tf.GradientTape()`,\n",
    "2. Create the variable within the training step function.\n",
    "\n",
    "```python\n",
    "c1 = ltn.Constant([2.1,3], trainable=True)\n",
    "c2 = ltn.Constant([4.5,0.8], trainable=True)\n",
    "# Do not assign the variable here. Tensorflow would not keep track of the \n",
    "# gradients between c1/c2 and x during training.\n",
    "...\n",
    "@tf.function\n",
    "def axioms():\n",
    "    # The assignation must be done within the tf.GradientTape, \n",
    "    # inside of the training step function.\n",
    "    x = ltn.Variable.from_constants(\"x\", [c1,c2])\n",
    "    return Forall(x,P(x))\n",
    "...\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = 1. - axioms() # the training step function is called within the tape.\n",
    "    grads = tape.gradient(loss_value, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "889985fd10eb245a43f2ae5f5aa0c555254f5b898fe16071f1c89d06fa8d76a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('tf-py39': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
